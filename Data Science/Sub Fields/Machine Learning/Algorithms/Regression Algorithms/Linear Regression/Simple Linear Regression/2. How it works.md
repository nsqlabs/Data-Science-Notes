Linear regression, at it’s core, is a way of calculating the relationship between two variables. It assumes that there’s a direct correlation between the two variables, and that this relationship can be represented with a straight line.

These two variables are called the _independent variable_ and the _dependent variable_, and they are given these names for fairly intuitive reasons. The _independent variable_ is so named because the model assumes that it can behave however it likes, and doesn’t depend on the other variable for any reason. The _dependent variable_ is the opposite; the model assumes that it is a direct result of the _independent variable_, it’s value is highly dependent on the _independent variable_.

<mark style="background: #FF5582A6;">Linear regression creates a linear mathematical relationships between these two variables. It enables calculation predicting the _dependent variable_ if the _dependent variable_ is known.</mark>

Since it’s such a simple form of regression, the governing equation for linear regression is also quite simple. It takes the form:

$$y = B* x + A$$

## How are the coefficients calculated?

Essentially, the **coefficients A and B are calculated to minimize the error between the models predictions and the actual data in the training set**.

Keep in mind that error between the data and the predictions is calculated as follows:

Error = Actual — Prediction

Therefore, minimizing the error between the model predictions and the actual data means performing the following steps for each x value in your data set:

-   First, use the linear regression equation, with values for A and B, to calculate predictions for each value of x;
-   Second, calculate the error for each value of x by subtracting the prediction for that x from the actual, known data;
-   Third, sum the error of all of the points to identify the total error from a linear regression equation using those values for A and B.

Those are the basic steps.

```ad-caution
<mark>Keep in mind that some errors would be positive, while others would be negative. These errors would cancel each other out and bring the resulting error closer to 0, despite there being error in both readings. Take for instance two points, one with error of 5 and the other with error of -10. While we all know that both these two points should be considered as causing 15 total points of error, the method described above treats them as causing -5 points of error. </mark>
```

To overcome this problem, **algorithms developing linear regression models use the _squared error_ instead of simply the error.** In other words, the formula for calculating the error takes the form:

Error = (Actual — Prediction)²

<mark style="background: #FF5582A6;">Since negative values squared will always return positive values, this prevents the errors canceling each other out and making bad models appear accurate.</mark> 

<mark style="background: #FF5582A6;">Since the linear regression model minimizes the _squared error_ the solution is referred to as the _least squares solution_. This is the name for the combination of A and B that return the minimum _squared error_ over the data set.</mark> Guessing and checking A and B would be extremely tedious. Using an optimization algorithm is another possibility, but would probably be time consuming and annoying. Fortunately, mathematicians have found an algebraic solution to this problem. The _least squares solution_ can be found using the following two equations:

$B = correlation(x,y) * \mu(y) / \mu(x)$
$A = mean(y) — B * mean(x)$

where:
- $μ$ represents the standard deviations
- mean represents the average or mean of y values in the data set
- correlation is a value representing the strength of correlation between the two.

```ad-hint
If you’re doing this work in the python package `pandas` you’ll be able to use the `DataFrame.mean()` function to identify mean(y) and `numpy` has a function to find the correlation.
```

The fact that these two equations return the _least squares solution_ isn’t incredibly intuitive, but we can make sense of it pretty quickly.
- Look at the equation for A. It essentially states that we need a value which returns the average value of y (The _dependent variable_) when when given the average value of x (The _independent variable_). It’s trying to create a line that runs through the center of the data set.
- Now look at the equation for B. It states that the value of the _dependent variable_ y should change by the standard deviation of y times the correlation between the two variables when the value of the _independent variable_ changes by the standard deviation of x. 

```ad-abstract
This is a very wordy way of saying that the two values each change by 1 standard deviation times the correlation between the two.
```