Feature selection refers to techniques for selecting a subset of input features that are most relevant to the target variable that is being predicted.

This is important as irrelevant and redundant input variables can distract or mislead learning algorithms possibly resulting in lower predictive performance. Additionally, it is desirable to develop models only using the data that is required to make a prediction, e.g. to favor the simplest possible well performing model.

**Feature selection techniques are generally grouped into those that use the target variable (**supervised**) and those that do not (**unsupervised**).** 
Additionally, the supervised techniques can be further divided into:

- Models that automatically select features as part of fitting the model (**intrinsic**).
- Those that explicitly choose features that result in the best performing model (**wrapper**).
- Those that score each input feature and allow a subset to be selected (**filter**).

![Overview of Feature Selection Techniques](https://machinelearningmastery.com/wp-content/uploads/2019/11/Overview-of-Feature-Selection-Techniques3.png)

==Statistical methods are popular for scoring input features, such as correlation. The features can then be ranked by their scores and a subset with the largest scores used as input to a model.== The choice of statistical measure depends on the data types of the input variables and a review of different statistical measures that can be used.

Additionally, there are different common feature selection use cases we may encounter in a predictive modeling project, such as:

-   Categorical inputs for a classification target variable.
-   Numerical inputs for a classification target variable.
-   Numerical inputs for a regression target variable.

When a mixture of input variable data types a present, different filter methods can be used. Alternately, a wrapper method such as the popular RFE method can be used that is agnostic to the input variable type.

The broader field of scoring the relative importance of input features is referred to as feature importance and many model-based techniques exist whose outputs can be used to aide in interpreting the model, interpreting the dataset, or in selecting features for modeling.

# Checklist

- [ ] **Do you have domain knowledge?** If yes, construct a better set of ad hoc”” features
- [ ] **Are your features commensurate?** If no, consider normalizing them.
- [ ] **Do you suspect interdependence of features?** If yes, expand your feature set by constructing conjunctive features or products of features, as much as your computer resources allow you.
- [ ] **Do you need to prune the input variables (e.g. for cost, speed or data understanding reasons)?** If no, construct disjunctive features or weighted sums of feature
- [ ] **Do you need to assess features individually (e.g. to understand their influence on the system or because their number is so large that you need to do a first filtering)?** If yes, use a variable ranking method; else, do it anyway to get baseline results.
- [ ] **Do you need a predictor?** If no, stop
- [ ] **Do you suspect your data is “dirty” (has a few meaningless input patterns and/or noisy outputs or wrong class labels)?** If yes, detect the outlier examples using the top ranking variables obtained in step 5 as representation; check and/or discard them.
- [ ] **Do you know what to try first?** If no, use a linear predictor. Use a forward selection method with the “probe” method as a stopping criterion or use the 0-norm embedded method for comparison, following the ranking of step 5, construct a sequence of predictors of same nature using increasing subsets of features. Can you match or improve performance with a smaller subset? If yes, try a non-linear predictor with that subset.
- [ ] **Do you have new ideas, time, computational resources, and enough examples?** If yes, compare several feature selection methods, including your new idea, correlation coefficients, backward selection and embedded methods. Use linear and non-linear predictors. Select the best approach with model selection
- [ ] **Do you want a stable solution (to improve performance and/or understanding)?** If yes, subsample your data and redo your analysis for several “bootstrap”.