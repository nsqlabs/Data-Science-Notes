Machine learning algorithms are parametrized so that you can tailor their behavior and results to your problem.

The problem is that “_how_” to do the tailoring is rarely (if ever) explained. Often, it is poorly understood, even by the algorithm developers themselves.

Generally, machine learning algorithms with stochastic elements are complex systems and must be studied as such. The first order – what effect does the parameter have on the complex system may be described. If so, you may have available some heuristics on how to configure the algorithm as a system.

It is the second order, what effect will it have on your results which is not known. Sometimes you can talk in generalities about the parameters’ effects on the algorithm as a system and how this translates to classes of problem, often not.

### No Best Algorithm Parameters

New sets of algorithm configurations are essentially new instances of algorithms for you to challenge your problem (albeit, relatively constrained or similar in the results they can achieve).

You cannot know the best algorithm parameters for your problem _a priori_.

-   You can use the parameters used in the seminal paper.
-   You can use the parameters in a book.
-   You can use the parameters listed in a “_how I did it_” kaggle post.

Good rules of thumb. Right? Maybe, maybe not.