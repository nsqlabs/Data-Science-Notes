Here is where Kaggle can teach us something. Solid feature engineering can get you a very high score on the leaderboard with a relatively simple learning algorithm. Inexperienced data scientists may think that fancy model stacking and hyperparameter tuning is where all the edge is to be had, but those are mainly techniques that the winners of Kaggle competitions spend a lot of time on to gain those last few decimal points on the score. In a real-world setting, this fanciness is not cost-effective and only contributes to added complexity. Feature engineering however, can greatly improve results.